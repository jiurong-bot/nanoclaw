# NanoClaw AI æ¨¡å‹ç®¡ç†ç³»çµ±

## æ¦‚è¿°

ç‚º NanoClaw æ·»åŠ éˆæ´»çš„ AI æ¨¡å‹åˆ‡æ›èƒ½åŠ›ï¼š
- âœ… æ”¯æŒå¤šå€‹ AI æä¾›å•†ï¼ˆGroqã€OpenAIã€Anthropicï¼‰
- âœ… é‹è¡Œæ™‚å‹•æ…‹åˆ‡æ›æ¨¡å‹
- âœ… è‡ªå‹•æ€§èƒ½è©•æ¸¬èˆ‡é¸æ“‡æœ€å„ªæ¨¡å‹
- âœ… Telegram å‘½ä»¤æ§åˆ¶
- âœ… æ¨¡å‹ä½¿ç”¨è¨˜éŒ„èˆ‡çµ±è¨ˆ

---

## Part 1ï¼šé…ç½®å¤šå€‹ AI æä¾›å•†

### Step 1ï¼šæ›´æ–° .env æ–‡ä»¶

```bash
nano .env
```

**é…ç½®å…§å®¹ï¼š**

```
# ========== ä¸»å‹• AI é…ç½® ==========
ACTIVE_AI_PROVIDER=groq
ACTIVE_AI_MODEL=mixtral-8x7b-32768

# ========== Groq é…ç½® ==========
GROQ_API_KEY=ä½ çš„_groq_api_key
GROQ_MODELS=mixtral-8x7b-32768,llama-3.3-70b-versatile,llama-2-70b-chat

# ========== OpenAI é…ç½®ï¼ˆå¯é¸ï¼‰ ==========
OPENAI_API_KEY=ä½ çš„_openai_api_key
OPENAI_MODELS=gpt-4,gpt-4-turbo,gpt-3.5-turbo

# ========== Anthropic é…ç½®ï¼ˆå¯é¸ï¼‰ ==========
ANTHROPIC_API_KEY=ä½ çš„_anthropic_api_key
ANTHROPIC_MODELS=claude-3-opus,claude-3-sonnet,claude-3-haiku

# ========== æ¨¡å‹è©•åˆ†èˆ‡è‡ªé©æ‡‰ ==========
MODEL_AUTO_SELECT=true
MODEL_PERFORMANCE_THRESHOLD=0.7
MODEL_FALLBACK_ENABLED=true

# ========== å…¶ä»–é…ç½® ==========
TAVILY_API_KEY=ä½ çš„_tavily_key
TELEGRAM_BOT_TOKEN=ä½ çš„_bot_token
NODE_ENV=production
```

**èªªæ˜ï¼š**
- `ACTIVE_AI_PROVIDER`ï¼šç•¶å‰ä¸»è¦ä½¿ç”¨çš„æä¾›å•†
- `ACTIVE_AI_MODEL`ï¼šç•¶å‰ä¸»è¦ä½¿ç”¨çš„æ¨¡å‹
- `MODEL_AUTO_SELECT`ï¼šæ˜¯å¦è‡ªå‹•é¸æ“‡æœ€å„ªæ¨¡å‹
- `MODEL_FALLBACK_ENABLED`ï¼šæ¨¡å‹å¤±æ•—æ™‚è‡ªå‹•åˆ‡æ›å‚™é¸

---

## Part 2ï¼šå®‰è£ä¾è³´

### Step 2ï¼šå®‰è£ AI SDK

```bash
cd /root/nanoclaw

# å®‰è£æ‰€æœ‰ AI æä¾›å•† SDK
npm install @groq-cloud/sdk openai @anthropic-ai/sdk

# å·²æœ‰ä¾è³´
npm install @tavily/core schedule node-telegram-bot-api
```

---

## Part 3ï¼šå¯¦ç¾æ¨¡å‹ç®¡ç†å™¨

### Step 3ï¼šå‰µå»º src/models/model-manager.ts

```bash
mkdir -p src/models
nano src/models/model-manager.ts
```

**è¤‡è£½ä»¥ä¸‹ä»£ç¢¼ï¼š**

```typescript
import Groq from '@groq-cloud/sdk';
import OpenAI from 'openai';
import Anthropic from '@anthropic-ai/sdk';

interface AIProvider {
  name: string;
  client: any;
  models: string[];
  apiKey: string;
  enabled: boolean;
}

interface ModelStats {
  name: string;
  provider: string;
  successCount: number;
  failureCount: number;
  avgResponseTime: number;
  score: number;
}

class ModelManager {
  private currentProvider: string;
  private currentModel: string;
  private providers: Map<string, AIProvider> = new Map();
  private modelStats: Map<string, ModelStats> = new Map();
  private autoSelect: boolean;
  private fallbackEnabled: boolean;

  constructor() {
    this.currentProvider = process.env.ACTIVE_AI_PROVIDER || 'groq';
    this.currentModel = process.env.ACTIVE_AI_MODEL || 'mixtral-8x7b-32768';
    this.autoSelect = process.env.MODEL_AUTO_SELECT === 'true';
    this.fallbackEnabled = process.env.MODEL_FALLBACK_ENABLED === 'true';

    this.initializeProviders();
    console.log(`ğŸ¤– æ¨¡å‹ç®¡ç†å™¨å·²åˆå§‹åŒ–\nç•¶å‰æ¨¡å‹ï¼š${this.currentProvider}/${this.currentModel}`);
  }

  /**
   * åˆå§‹åŒ–æ‰€æœ‰ AI æä¾›å•†
   */
  private initializeProviders(): void {
    // Groq
    if (process.env.GROQ_API_KEY) {
      this.providers.set('groq', {
        name: 'Groq',
        client: new Groq({ apiKey: process.env.GROQ_API_KEY }),
        models: (process.env.GROQ_MODELS || 'mixtral-8x7b-32768').split(','),
        apiKey: process.env.GROQ_API_KEY,
        enabled: true
      });
      console.log('âœ… Groq å·²åˆå§‹åŒ–');
    }

    // OpenAI
    if (process.env.OPENAI_API_KEY) {
      this.providers.set('openai', {
        name: 'OpenAI',
        client: new OpenAI({ apiKey: process.env.OPENAI_API_KEY }),
        models: (process.env.OPENAI_MODELS || 'gpt-4').split(','),
        apiKey: process.env.OPENAI_API_KEY,
        enabled: true
      });
      console.log('âœ… OpenAI å·²åˆå§‹åŒ–');
    }

    // Anthropic
    if (process.env.ANTHROPIC_API_KEY) {
      this.providers.set('anthropic', {
        name: 'Anthropic',
        client: new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY }),
        models: (process.env.ANTHROPIC_MODELS || 'claude-3-sonnet').split(','),
        apiKey: process.env.ANTHROPIC_API_KEY,
        enabled: true
      });
      console.log('âœ… Anthropic å·²åˆå§‹åŒ–');
    }
  }

  /**
   * ç²å– AI å›è¦†ï¼ˆæ”¯æŒè‡ªå‹•é™ç´šï¼‰
   */
  async generateResponse(prompt: string, retryCount = 0): Promise<string> {
    try {
      const startTime = Date.now();
      const response = await this.callModel(prompt);
      const responseTime = Date.now() - startTime;

      // è¨˜éŒ„çµ±è¨ˆ
      this.recordSuccess(this.currentProvider, this.currentModel, responseTime);

      return response;
    } catch (error) {
      console.error(`âŒ ${this.currentProvider}/${this.currentModel} å¤±æ•—:`, error);
      this.recordFailure(this.currentProvider, this.currentModel);

      // å˜—è©¦é™ç´šåˆ°å‚™é¸æ¨¡å‹
      if (this.fallbackEnabled && retryCount < 2) {
        const fallback = this.findFallbackModel();
        if (fallback) {
          console.log(`ğŸ”„ è‡ªå‹•é™ç´šåˆ°ï¼š${fallback.provider}/${fallback.model}`);
          this.setActiveModel(fallback.provider, fallback.model);
          return this.generateResponse(prompt, retryCount + 1);
        }
      }

      throw error;
    }
  }

  /**
   * èª¿ç”¨ç•¶å‰æ¨¡å‹
   */
  private async callModel(prompt: string): Promise<string> {
    const provider = this.providers.get(this.currentProvider);
    if (!provider) {
      throw new Error(`æä¾›å•†ä¸å­˜åœ¨ï¼š${this.currentProvider}`);
    }

    switch (this.currentProvider) {
      case 'groq':
        return this.callGroq(provider, prompt);
      case 'openai':
        return this.callOpenAI(provider, prompt);
      case 'anthropic':
        return this.callAnthropic(provider, prompt);
      default:
        throw new Error(`æœªçŸ¥æä¾›å•†ï¼š${this.currentProvider}`);
    }
  }

  /**
   * Groq èª¿ç”¨
   */
  private async callGroq(provider: AIProvider, prompt: string): Promise<string> {
    const message = await provider.client.messages.create({
      model: this.currentModel,
      max_tokens: 1024,
      messages: [{ role: 'user', content: prompt }]
    });
    return message.content[0].type === 'text' ? message.content[0].text : '';
  }

  /**
   * OpenAI èª¿ç”¨
   */
  private async callOpenAI(provider: AIProvider, prompt: string): Promise<string> {
    const message = await provider.client.chat.completions.create({
      model: this.currentModel,
      max_tokens: 1024,
      messages: [{ role: 'user', content: prompt }]
    });
    return message.choices[0].message.content || '';
  }

  /**
   * Anthropic èª¿ç”¨
   */
  private async callAnthropic(provider: AIProvider, prompt: string): Promise<string> {
    const message = await provider.client.messages.create({
      model: this.currentModel,
      max_tokens: 1024,
      messages: [{ role: 'user', content: prompt }]
    });
    return message.content[0].type === 'text' ? message.content[0].text : '';
  }

  /**
   * è¨­ç½®æ´»èºæ¨¡å‹
   */
  setActiveModel(provider: string, model: string): boolean {
    const p = this.providers.get(provider);
    if (!p || !p.models.includes(model)) {
      return false;
    }

    this.currentProvider = provider;
    this.currentModel = model;
    console.log(`âœ… å·²åˆ‡æ›è‡³ï¼š${provider}/${model}`);
    return true;
  }

  /**
   * è‡ªå‹•é¸æ“‡æœ€å„ªæ¨¡å‹
   */
  async selectBestModel(): Promise<{ provider: string; model: string }> {
    if (!this.autoSelect) {
      return { provider: this.currentProvider, model: this.currentModel };
    }

    let bestScore = 0;
    let bestChoice = { provider: this.currentProvider, model: this.currentModel };

    for (const [providerName, provider] of this.providers) {
      if (!provider.enabled) continue;

      for (const model of provider.models) {
        const stats = this.modelStats.get(`${providerName}/${model}`);
        const score = stats ? stats.score : 0.5; // æ–°æ¨¡å‹é»˜èª 0.5 score

        if (score > bestScore) {
          bestScore = score;
          bestChoice = { provider: providerName, model };
        }
      }
    }

    if (bestChoice.provider !== this.currentProvider || bestChoice.model !== this.currentModel) {
      this.setActiveModel(bestChoice.provider, bestChoice.model);
    }

    return bestChoice;
  }

  /**
   * è¨˜éŒ„æˆåŠŸ
   */
  private recordSuccess(provider: string, model: string, responseTime: number): void {
    const key = `${provider}/${model}`;
    const stats = this.modelStats.get(key) || {
      name: model,
      provider,
      successCount: 0,
      failureCount: 0,
      avgResponseTime: 0,
      score: 0.5
    };

    stats.successCount++;
    stats.avgResponseTime = (stats.avgResponseTime + responseTime) / 2;
    stats.score = Math.min(
      (stats.successCount / (stats.successCount + stats.failureCount + 1)) * 
      (1 - Math.min(stats.avgResponseTime / 10000, 0.3)),
      1
    );

    this.modelStats.set(key, stats);
  }

  /**
   * è¨˜éŒ„å¤±æ•—
   */
  private recordFailure(provider: string, model: string): void {
    const key = `${provider}/${model}`;
    const stats = this.modelStats.get(key) || {
      name: model,
      provider,
      successCount: 0,
      failureCount: 0,
      avgResponseTime: 0,
      score: 0.5
    };

    stats.failureCount++;
    stats.score = Math.max(
      (stats.successCount / (stats.successCount + stats.failureCount + 1)) * 0.8,
      0.1
    );

    this.modelStats.set(key, stats);
  }

  /**
   * å°‹æ‰¾å‚™é¸æ¨¡å‹
   */
  private findFallbackModel(): { provider: string; model: string } | null {
    let bestStats: { provider: string; model: string; score: number } | null = null;

    for (const [key, stats] of this.modelStats) {
      if (stats.score > (bestStats?.score || 0) && 
          `${stats.provider}/${stats.name}` !== `${this.currentProvider}/${this.currentModel}`) {
        bestStats = {
          provider: stats.provider,
          model: stats.name,
          score: stats.score
        };
      }
    }

    if (bestStats && bestStats.score >= 0.5) {
      return { provider: bestStats.provider, model: bestStats.model };
    }

    // å¦‚æœæ²’æœ‰é”æ¨™çš„å‚™é¸ï¼Œå°±é¸ç¬¬ä¸€å€‹å¯ç”¨æä¾›å•†
    for (const [providerName, provider] of this.providers) {
      if (provider.enabled && provider.models.length > 0 && 
          providerName !== this.currentProvider) {
        return { provider: providerName, model: provider.models[0] };
      }
    }

    return null;
  }

  /**
   * åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹
   */
  listAvailableModels(): string {
    let list = 'ğŸ¤– å¯ç”¨ AI æ¨¡å‹åˆ—è¡¨ï¼š\n\n';

    for (const [providerName, provider] of this.providers) {
      if (!provider.enabled) continue;

      list += `**${provider.name}**\n`;
      for (const model of provider.models) {
        const key = `${providerName}/${model}`;
        const stats = this.modelStats.get(key);
        const marker = this.currentProvider === providerName && this.currentModel === model ? 'âœ¨' : '  ';
        list += `${marker} ${model}`;

        if (stats) {
          list += ` (æˆåŠŸç‡: ${((stats.successCount / (stats.successCount + stats.failureCount + 1)) * 100).toFixed(0)}%)`;
        }
        list += '\n';
      }
      list += '\n';
    }

    return list;
  }

  /**
   * ç²å–æ¨¡å‹çµ±è¨ˆ
   */
  getStats(): string {
    let stats = `ğŸ“Š AI æ¨¡å‹çµ±è¨ˆ\n\n`;
    stats += `ç•¶å‰æ¨¡å‹ï¼š**${this.currentProvider}/${this.currentModel}**\n\n`;
    stats += `**è©³ç´°çµ±è¨ˆï¼š**\n`;

    for (const [key, stat] of this.modelStats) {
      const successRate = ((stat.successCount / (stat.successCount + stat.failureCount + 1)) * 100).toFixed(0);
      stats += `${key}\n  æˆåŠŸï¼š${stat.successCount} | å¤±æ•—ï¼š${stat.failureCount} | æˆåŠŸç‡ï¼š${successRate}% | è©•åˆ†ï¼š${stat.score.toFixed(2)}\n`;
    }

    return stats;
  }

  /**
   * ç²å–ç•¶å‰æ´»èºæ¨¡å‹info
   */
  getCurrentModel(): { provider: string; model: string } {
    return { provider: this.currentProvider, model: this.currentModel };
  }
}

export default new ModelManager();
```

---

## Part 4ï¼šæ•´åˆåˆ°ä¸»æ‡‰ç”¨

### Step 4ï¼šæ›´æ–° src/index.ts

```bash
nano src/index.ts
```

**æ·»åŠ æ¨¡å‹ç®¡ç†ä»£ç¢¼ç‰‡æ®µï¼š**

```typescript
import TelegramBot from 'node-telegram-bot-api';
import modelManager from './models/model-manager';
import skillFinder from './skills/skill-finder';

const tgBot = new TelegramBot(process.env.TELEGRAM_BOT_TOKEN, { polling: true });

// ========== æ¨¡å‹é¸æ“‡å‘½ä»¤ ==========

// åˆ—å‡ºæ‰€æœ‰æ¨¡å‹
tgBot.onText(/\/models/, async (msg) => {
  const chatId = msg.chat.id;
  const list = modelManager.listAvailableModels();
  await tgBot.sendMessage(chatId, list);
});

// åˆ‡æ›åˆ°æŒ‡å®šæ¨¡å‹
tgBot.onText(/\/switch_model (.+) (.+)/, async (msg, match) => {
  const chatId = msg.chat.id;
  const provider = match[1];
  const model = match[2];

  const success = modelManager.setActiveModel(provider, model);
  const message = success
    ? `âœ… å·²åˆ‡æ›è‡³ ${provider}/${model}`
    : `âŒ æ¨¡å‹ä¸å­˜åœ¨æˆ–æä¾›å•†æœªå•Ÿç”¨\nè«‹ç”¨ /models æŸ¥çœ‹å¯ç”¨æ¨¡å‹`;
  
  await tgBot.sendMessage(chatId, message);
});

// è‡ªå‹•é¸æ“‡æœ€å„ªæ¨¡å‹
tgBot.onText(/\/auto_select/, async (msg) => {
  const chatId = msg.chat.id;
  const best = await modelManager.selectBestModel();
  const message = `âœ… å·²è‡ªå‹•é¸æ“‡æœ€å„ªæ¨¡å‹\n${best.provider}/${best.model}`;
  await tgBot.sendMessage(chatId, message);
});

// æŸ¥çœ‹æ¨¡å‹çµ±è¨ˆ
tgBot.onText(/\/model_stats/, async (msg) => {
  const chatId = msg.chat.id;
  const stats = modelManager.getStats();
  await tgBot.sendMessage(chatId, stats);
});

// ========== ä¸»èŠå¤©é‚è¼¯ï¼ˆä½¿ç”¨æ¨¡å‹ç®¡ç†å™¨ï¼‰ ==========

tgBot.onText(/(.+)/, async (msg, match) => {
  const chatId = msg.chat.id;
  const userMessage = match[1];

  try {
    // é¦–å…ˆå˜—è©¦æ‰¾æŠ€èƒ½
    const skillMatch = await skillFinder.findBestSkill(userMessage);
    if (skillMatch && skillMatch.confidence > 0.5) {
      const result = await skillMatch.skill.execute(userMessage);
      await tgBot.sendMessage(chatId, result);
      return;
    }

    // ä½¿ç”¨ AI æ¨¡å‹å›è¦†
    const response = await modelManager.generateResponse(userMessage);
    await tgBot.sendMessage(chatId, response);
  } catch (error) {
    console.error('Error:', error);
    await tgBot.sendMessage(chatId, 'âŒ ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œé‡è©¦');
  }
});

console.log('ğŸš€ NanoClaw with Model Manager å·²å•Ÿå‹•');
```

---

## Part 5ï¼šTelegram å‘½ä»¤æŒ‡å—

### å¯ç”¨å‘½ä»¤

```
/models                          - åˆ—å‡ºæ‰€æœ‰å¯ç”¨ AI æ¨¡å‹
/switch_model [provider] [model] - åˆ‡æ›åˆ°æŒ‡å®šæ¨¡å‹
  ä¾‹ï¼š/switch_model groq mixtral-8x7b-32768
      /switch_model openai gpt-4
      /switch_model anthropic claude-3-opus

/auto_select                     - è‡ªå‹•é¸æ“‡æ€§èƒ½æœ€å„ªæ¨¡å‹
/model_stats                     - æŸ¥çœ‹å„æ¨¡å‹æ€§èƒ½çµ±è¨ˆ
```

### ä½¿ç”¨ç¤ºä¾‹

```
ç”¨æˆ¶ï¼š/models
Botå›è¦†ï¼š
ğŸ¤– å¯ç”¨ AI æ¨¡å‹åˆ—è¡¨ï¼š

**Groq**
âœ¨ mixtral-8x7b-32768 (æˆåŠŸç‡: 100%)
  llama-3.3-70b-versatile

**OpenAI**
  gpt-4

---

ç”¨æˆ¶ï¼š/switch_model openai gpt-4
Botå›è¦†ï¼šâœ… å·²åˆ‡æ›è‡³ openai/gpt-4

---

ç”¨æˆ¶ï¼š/auto_select
Botå›è¦†ï¼šâœ… å·²è‡ªå‹•é¸æ“‡æœ€å„ªæ¨¡å‹
gpt-4/openai

---

ç”¨æˆ¶ï¼š/model_stats
Botå›è¦†ï¼š
ğŸ“Š AI æ¨¡å‹çµ±è¨ˆ
ç•¶å‰æ¨¡å‹ï¼š**groq/mixtral-8x7b-32768**

è©³ç´°çµ±è¨ˆï¼š
groq/mixtral-8x7b-32768
  æˆåŠŸï¼š95 | å¤±æ•—ï¼š2 | æˆåŠŸç‡ï¼š98% | è©•åˆ†ï¼š0.96
openai/gpt-4
  æˆåŠŸï¼š5 | å¤±æ•—ï¼š0 | æˆåŠŸç‡ï¼š100% | è©•åˆ†ï¼š0.92
```

---

## è‡ªå‹•æ¨¡å‹åˆ‡æ›é‚è¼¯

### å·¥ä½œæµç¨‹

```
ç”¨æˆ¶æ¶ˆæ¯
   â†“
å˜—è©¦ç•¶å‰æ¨¡å‹
   â”œâ”€ æˆåŠŸ â†’ è¿”å›å›è¦†ï¼Œè¨˜éŒ„æˆåŠŸ
   â””â”€ å¤±æ•— â†’ è‡ªå‹•é™ç´š
       â†“
   æŸ¥æ‰¾å‚™é¸æ¨¡å‹ï¼ˆæ€§èƒ½è©•åˆ†æœ€é«˜ï¼‰
       â†“
   è‡ªå‹•åˆ‡æ›ä¸¦é‡è©¦
       â†“
   å¦‚æœé‚„å¤±æ•— â†’ è¿”å›éŒ¯èª¤ä¿¡æ¯
```

### æ¨¡å‹è©•åˆ†è¨ˆç®—

```
Score = (Success Rate) Ã— (1 - Response Time Factor) Ã— (Reliability Penalty)

ä¾‹å¦‚ï¼š
- æˆåŠŸç‡ 98% + å¹³å‡éŸ¿æ‡‰æ™‚é–“ 200ms + å¯é  = 0.96 åˆ†
- æˆåŠŸç‡ 100% + å¹³å‡éŸ¿æ‡‰æ™‚é–“ 5000ms = 0.70 åˆ†
```

---

## å®Œæ•´æª¢æŸ¥æ¸…å–®

- [ ] npm ä¾è³´å·²å®‰è£ï¼ˆ@groq-cloud/sdk, openai, @anthropic-ai/sdkï¼‰
- [ ] .env é…ç½®å®Œæ•´ï¼ˆæ‰€æœ‰ API Keyï¼‰
- [ ] src/models/model-manager.ts å·²å‰µå»º
- [ ] src/index.ts å·²æ›´æ–°æ¨¡å‹ç®¡ç†ä»£ç¢¼
- [ ] npm start æˆåŠŸé‹è¡Œ
- [ ] Telegram æ¸¬è©¦ /models å‘½ä»¤
- [ ] Telegram æ¸¬è©¦ /switch_model å‘½ä»¤
- [ ] Telegram æ¸¬è©¦ /auto_select å‘½ä»¤
- [ ] Telegram æ¸¬è©¦ /model_stats å‘½ä»¤

---

## æˆæœ¬ç®¡ç†å»ºè­°

```
ç”±æ–¼æ”¯æŒå¤šå€‹ä»˜è²» APIï¼Œå»ºè­°è¨­ç½®å„ªå…ˆç´šï¼š

1ï¸âƒ£ Groqï¼ˆæ¨è–¦ï¼Œå…è²»ï¼‰
2ï¸âƒ£ å‚™é¸ä»˜è²»æœå‹™ï¼ˆOpenAI/Anthropicï¼‰

é…ç½®å„ªå…ˆç´šï¼š
ACTIVE_AI_PROVIDER=groq      // å„ªå…ˆç”¨å…è²»çš„
MODEL_FALLBACK_ENABLED=true  // å¤±æ•—æ™‚æ‰ç”¨ä»˜è²»çš„
```

---

**ç¾åœ¨å¯ä»¥éˆæ´»åˆ‡æ› AI æ¨¡å‹äº†ï¼** ğŸ¤–
